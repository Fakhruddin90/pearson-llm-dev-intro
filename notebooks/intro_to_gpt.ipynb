{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1898f1",
   "metadata": {},
   "source": [
    "## Introduction to the GPT family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from torch import tensor, numel\n",
    "from bertviz import model_view, head_view\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe869922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline object in transformers provides easy access to transformer usage\n",
    "MODEL = 'gpt2'\n",
    "\n",
    "generator = pipeline('text-generation', model=MODEL)\n",
    "\n",
    "# finish the sentence\n",
    "generator(\"Hello, I'm a language model and I\", max_length=30, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "'Sinan' in tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dce33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('Sinan loves a beautiful day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a string and then convert the ids back into tokens. Note the Ä  character denoting a space before the token\n",
    "tokenizer.convert_ids_to_tokens(tokenizer.encode('Sinan loves a beautiful day'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('Sinan loves a beautiful day')  # ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c892447",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode('Sinan loves a beautiful day', return_tensors='pt')  # as a pytorch tensor\n",
    "\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bee3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up a tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68795152",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.wte(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.wte(encoded).shape  # 1 item in batch x 6 tokens x token dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.wpe(tensor([0, 1, 2, 3, 4, 5]).reshape(1, 6)).shape  # manually create position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8200f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GPT input\n",
    "initial_input = model.transformer.wte(encoded) + model.transformer.wpe(tensor([0, 1, 2, 3, 4, 5]).reshape(1, 6))\n",
    "\n",
    "initial_input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a70fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input = model.transformer.drop(initial_input)  # run our input through the model's initual dropout later\n",
    "initial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in model.transformer.h:  # run the initial_input through every decoder in the stack\n",
    "    initial_input = module(initial_input)[0]\n",
    "    \n",
    "initial_input = model.transformer.ln_f(initial_input)  # and then the final layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11780d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as just running through the model\n",
    "(initial_input == model(encoded, output_hidden_states=True).hidden_states[-1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(encoded).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    total_params += numel(param)\n",
    "    \n",
    "print(f'Number of params: {total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7ed76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "274aac4e",
   "metadata": {},
   "source": [
    "## Masked multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1da310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'My friend was right about this class. It is so fun!'\n",
    "encoded_phrase = tokenizer(phrase, return_tensors='pt')\n",
    "\n",
    "response = model(**encoded_phrase, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "len(response.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe66d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.attentions[-1].shape  # represtnations from the final decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2337791",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_phrase['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eec46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_phrase['input_ids'][0])\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer index 9, head 0. Check out the almost 60% attention the token it is giving to the token class\n",
    "arr = response.attentions[9][0][0]\n",
    "\n",
    "n_digits = 3\n",
    "\n",
    "attention_df = pd.DataFrame((torch.round(arr * 10**n_digits) / (10**n_digits)).detach()).applymap(float)\n",
    "\n",
    "attention_df.columns = tokens\n",
    "attention_df.index = tokens\n",
    "\n",
    "attention_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda9917",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "head_view(response.attentions, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4126c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc57a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_phrase['input_ids'][0]) \n",
    "model_view(response.attentions, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82dc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5760c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f80594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the top next token in the auto-regressive language modelling task\n",
    "pd.DataFrame(\n",
    "    zip(tokens, tokenizer.convert_ids_to_tokens(response.logits.argmax(2)[0])), \n",
    "    columns=['Sequence up until', 'Next token with highest probability']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd539ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator('My friend was right', max_length=4, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(phrase, max_length=20, num_return_sequences=1, do_sample=False)  # greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa15fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(phrase, max_length=20, num_return_sequences=1, do_sample=True)  # greedy search with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff934358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643fc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4616e0f",
   "metadata": {},
   "source": [
    "## Pre-training GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from torch import tensor\n",
    "\n",
    "generator = pipeline('text-generation', model=MODEL, tokenizer=tokenizer)\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768633e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982bb3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bias\n",
    "generator(\"The holocaust was\", max_length=10, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"Jewish people are\", max_length=10, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"Christian people are\", max_length=10, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"The earth is\", max_length=10, num_return_sequences=10, temperature=0.8, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93acda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ec88e66",
   "metadata": {},
   "source": [
    "## Few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60177cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generator(\"\"\"Sentiment Analysis\n",
    "Text: I hate it when my phone battery dies.\n",
    "Sentiment: Negative\n",
    "###\n",
    "Text: My day has been really great!\n",
    "Sentiment: Positive\n",
    "###\n",
    "Text: Not a fan when it is cloudy\n",
    "Sentiment:\"\"\", top_k=2, temperature=0.1, max_length=55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generator(\"\"\"Question/Answering\n",
    "C: Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock.\n",
    "Q: When was Google founded?\n",
    "A: 1998\n",
    "###\n",
    "C: Hugging Face is a company which develops social AI-run chatbot applications. It was established in 2016 by Clement Delangue and Julien Chaumond. The company is based in Brooklyn, New York, United States.\n",
    "Q: What does Hugging Face develop?\n",
    "A: social AI-run chatbot applications\n",
    "###\n",
    "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
    "Q: What division do the Jets play in?\n",
    "A:\"\"\", top_k=5, num_beams=2, max_length=215, temperature=0.5)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c405c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same question as before, with no previous examples ie Zero-shot learning. Hit or miss\n",
    "print(generator(\n",
    "    '''Question/Answering\n",
    "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
    "Q: What division do the Jets play in?\n",
    "A:''',\n",
    "    top_k=5, num_beams=2, max_length=80, temperature=0.5)[0]['generated_text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot doesn't work as much with the sentiment analysis example\n",
    "print(generator(\"\"\"Sentiment Analysis\n",
    "Text: This new music video was so good\n",
    "Sentiment:\"\"\", top_k=2, temperature=0.1, max_length=55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749594b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f678731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_summarize = \"\"\"This training will focus on how the GPT family of models are used for NLP tasks including abstractive text summarization and natural language generation. The training will begin with an introduction to necessary concepts including masked self attention, language models, and transformers and then build on those concepts to introduce the GPT architecture. We will then move into how GPT is used for multiple natural language processing tasks with hands-on examples of using pre-trained GPT-2 models as well as fine-tuning these models on custom corpora.\n",
    "\n",
    "GPT models are some of the most relevant NLP architectures today and it is closely related to other important NLP deep learning models like BERT. Both of these models are derived from the newly invented transformer architecture and represent an inflection point in how machines process language and context.\n",
    "\n",
    "The Natural Language Processing with Next-Generation Transformer Architectures series of online trainings provides a comprehensive overview of state-of-the-art natural language processing (NLP) models including GPT and BERT which are derived from the modern attention-driven transformer architecture and the applications these models are used to solve today. All of the trainings in the series blend theory and application through the combination of visual mathematical explanations, straightforward applicable Python examples within hands-on Jupyter notebook demos, and comprehensive case studies featuring modern problems solvable by NLP models. (Note that at any given time, only a subset of these classes will be scheduled and open for registration.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b05b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(generator(\n",
    "    f\"\"\"Summarization Task:\\n{to_summarize}\\nTL;DR:\"\"\", \n",
    "    max_length=400, num_beams=5, temperature=0.7\n",
    ")[0]['generated_text'].split('TL;DR:')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7258099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81773f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the GPT-2 model without the next token predictor (the causal part)\n",
    "model = AutoModel.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentence, word):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', add_special_tokens=True)\n",
    "    token_ids = inputs['input_ids'][0].tolist()\n",
    "    tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "    try:\n",
    "        # Find the first occurrence of 'word' in the list; this will give you an index.\n",
    "        token_idx = token_ids.index(tokenizer.encode(word)[0])\n",
    "    except ValueError:\n",
    "        print(f\"Word '{word}' not found in tokens.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Token index of {word} is {token_idx}\")\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[0][token_idx].detach().numpy().reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e9f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96c355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator(\"I love my pet python\", max_length=30, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defac58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f46d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "python_pet_embedding = get_embedding('I love my pet python', ' python')\n",
    "python_language_embedding = get_embedding('I love coding in python', ' python')\n",
    "snake_alone_embedding = get_embedding('snake', 'snake')\n",
    "coding_alone_embedding = get_embedding('coding', 'coding')\n",
    "\n",
    "# Calculate cosine similarities\n",
    "similarity_pet_snake = cosine_similarity(python_pet_embedding, snake_alone_embedding)\n",
    "similarity_language_snake = cosine_similarity(python_language_embedding, snake_alone_embedding)\n",
    "similarity_pet_coding = cosine_similarity(python_pet_embedding, coding_alone_embedding)\n",
    "similarity_language_coding = cosine_similarity(python_language_embedding, coding_alone_embedding)\n",
    "\n",
    "\n",
    "# Create the subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Chart for Pet Context\n",
    "labels_pet = ['Pet-Snake', 'Pet-Coding']\n",
    "values_pet = [similarity_pet_snake[0][0], similarity_pet_coding[0][0]]\n",
    "\n",
    "axs[0].barh(labels_pet, values_pet, color='skyblue')\n",
    "axs[0].set_xlabel('Cosine Similarity')\n",
    "axs[0].set_title('Cosine Similarity for \"Python\" in Pet Context (GPT-2)')\n",
    "for i, v in enumerate(values_pet):\n",
    "    axs[0].text(v, i, \" {:.2f}\".format(v), va='center', color='blue')\n",
    "\n",
    "# Chart for Language Context\n",
    "labels_language = ['Language-Snake', 'Language-Coding']\n",
    "values_language = [similarity_language_snake[0][0], similarity_language_coding[0][0]]\n",
    "\n",
    "axs[1].barh(labels_language, values_language, color='skyblue')\n",
    "axs[1].set_xlabel('Cosine Similarity')\n",
    "axs[1].set_title('Cosine Similarity for \"Python\" in Language Context (GPT-2)')\n",
    "for i, v in enumerate(values_language):\n",
    "    axs[1].text(v, i, \" {:.2f}\".format(v), va='center', color='blue')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't see that same stark difference like we did in BERT. For one, gpt2 isn't THAT good of a language model\n",
    "#  on it's own and secondly, auto-regressive models just aren't as good at this task at the same parameter level\n",
    "\n",
    "# GPT2 and bert have roughly the same number of parameters but bert is just WAY better at context clues than GPT2 is\n",
    "# GPT2 would need many more parameters to be as good as bert.... ie gpt 3, chatGPT , and gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embeddings\n",
    "python_pet_embedding = get_embedding('python, a programming language, is great.', 'python')\n",
    "python_language_embedding = get_embedding('python, and by python I mean the snake, is a super cool animal.', 'python')\n",
    "\n",
    "# When python is the first word, the embedding is exactly the same no matter what comes next\n",
    "(python_pet_embedding == python_language_embedding).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embeddings\n",
    "python_pet_embedding = get_embedding('What is a python? A programming language.', ' python')\n",
    "python_language_embedding = get_embedding('What is a python? A snake.', ' python')\n",
    "\n",
    "# Even with the words (albeit the same for each sentence) before python, the embedding is the same\n",
    "(python_pet_embedding == python_language_embedding).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_pet_embedding[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_language_embedding[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ca8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
